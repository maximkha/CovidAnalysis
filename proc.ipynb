{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import html\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from emolexHelper import pandasMultiCategorySentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_DIR = \"before_pandemic\"\n",
    "AFTER_DIR = \"since_pandemic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the csv files from\n",
    "#before = dict()\n",
    "df = pd.DataFrame(columns=[\"cat\", \"username\", \"text\", \"date\", \"geo\", \"hashtags\", \"tweet_id\", \"mentions\", \"permalink\", \"replies\", \"retweets\", \"replies_to\", \"mentioned_urls\"])\n",
    "\n",
    "for file in list((Path(\".\") / BEFORE_DIR).glob('**/*.csv')):\n",
    "    #before[file.stem] = pd.read_csv(file)\n",
    "    user_df = pd.read_csv(file)\n",
    "    # BUG? why are there nan text rows?\n",
    "    # dropping them!\n",
    "    user_df = user_df.drop(user_df[pd.isnull(user_df[\"text\"])].index)\n",
    "    user_df[\"cat\"] = BEFORE_DIR\n",
    "    user_df[\"username\"] = file.stem\n",
    "    df = df.append(user_df)\n",
    "    #before[file.stem] = before[file.stem].drop(before[file.stem][pd.isnull(before[file.stem][\"text\"])].index)\n",
    "\n",
    "#after = dict()\n",
    "for file in list((Path(\".\") / AFTER_DIR).glob('**/*.csv')):\n",
    "    #after[file.stem] = pd.read_csv(file)\n",
    "    user_df = pd.read_csv(file)\n",
    "    # BUG? why are there nan text rows?\n",
    "    # dropping them!\n",
    "    user_df = user_df.drop(user_df[pd.isnull(user_df[\"text\"])].index)\n",
    "    user_df[\"cat\"] = AFTER_DIR\n",
    "    user_df[\"username\"] = file.stem\n",
    "    df = df.append(user_df)\n",
    "    #after[file.stem] = after[file.stem].drop(after[file.stem][pd.isnull(after[file.stem][\"text\"])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before.keys()\n",
    "# before[\"1baldchick\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeAndClean(s:str):\n",
    "    out = html.unescape(s)\n",
    "    out = p.clean(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def getSentiment(s):\n",
    "    global analyzer\n",
    "    \n",
    "    try:\n",
    "        out = analyzer.polarity_scores(s)[\"compound\"]\n",
    "    except Exception:\n",
    "        print(s)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in filtered_words]\n",
    "    \n",
    "    return \" \".join(filtered_words) #convert stemmed words to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 cat    username  \\\n",
       "0    before_pandemic  1baldchick   \n",
       "1    before_pandemic  1baldchick   \n",
       "2    before_pandemic  1baldchick   \n",
       "3    before_pandemic  1baldchick   \n",
       "4    before_pandemic  1baldchick   \n",
       "..               ...         ...   \n",
       "270   since_pandemic  _tomdalton   \n",
       "271   since_pandemic  _tomdalton   \n",
       "272   since_pandemic  _tomdalton   \n",
       "273   since_pandemic  _tomdalton   \n",
       "274   since_pandemic  _tomdalton   \n",
       "\n",
       "                                                  text  \\\n",
       "0     big wet smooshy kisses to you!!! i love u dear!!   \n",
       "1    list of meds and check it for drug interaction...   \n",
       "2    gab has loads of side effects. i always try to...   \n",
       "3    happy friday to u good sir!! and to your littl...   \n",
       "4                               this song says it all!   \n",
       "..                                                 ...   \n",
       "270                          total bullshit narrative.   \n",
       "271  doctors are not always right either and many w...   \n",
       "272                                         yes x1000    \n",
       "273  very important thread. already hearing the anx...   \n",
       "274    is she trying to say he’s dumb or belittle him?   \n",
       "\n",
       "                          date  geo hashtags             tweet_id  \\\n",
       "0    2020-02-28 19:46:02+00:00  NaN      NaN  1233478483939971072   \n",
       "1    2020-02-28 19:45:26+00:00  NaN      NaN  1233478333511479299   \n",
       "2    2020-02-28 19:44:18+00:00  NaN      NaN  1233478047212408834   \n",
       "3    2020-02-28 19:42:32+00:00  NaN      NaN  1233477603866050560   \n",
       "4    2020-02-28 08:34:04+00:00  NaN      NaN  1233309379497283584   \n",
       "..                         ...  ...      ...                  ...   \n",
       "270  2020-03-07 03:27:53+00:00  NaN      NaN  1236131428892651520   \n",
       "271  2020-03-05 23:36:05+00:00  NaN      NaN  1235710707993374721   \n",
       "272  2020-03-05 18:57:13+00:00  NaN      NaN  1235640525740552192   \n",
       "273  2020-03-05 18:55:54+00:00  NaN      NaN  1235640197926264832   \n",
       "274  2020-03-02 01:30:36+00:00  NaN      NaN  1234289973467254785   \n",
       "\n",
       "          mentions                                          permalink replies  \\\n",
       "0              NaN  https://twitter.com/1baldchick/status/12334784...       0   \n",
       "1              NaN  https://twitter.com/1baldchick/status/12334783...       1   \n",
       "2              NaN  https://twitter.com/1baldchick/status/12334780...       1   \n",
       "3              NaN  https://twitter.com/1baldchick/status/12334776...       0   \n",
       "4              NaN  https://twitter.com/1baldchick/status/12333093...       0   \n",
       "..             ...                                                ...     ...   \n",
       "270            NaN  https://twitter.com/_tomdalton/status/12361314...       0   \n",
       "271            NaN  https://twitter.com/_tomdalton/status/12357107...       0   \n",
       "272            NaN  https://twitter.com/_tomdalton/status/12356405...       0   \n",
       "273  @MorganGodvin  https://twitter.com/_tomdalton/status/12356401...       1   \n",
       "274            NaN  https://twitter.com/_tomdalton/status/12342899...       0   \n",
       "\n",
       "    retweets      replies_to  \\\n",
       "0          0       amarcadia   \n",
       "1          0      1baldchick   \n",
       "2          0      PajamaDaze   \n",
       "3          0  ChronicPainDad   \n",
       "4          0          halsey   \n",
       "..       ...             ...   \n",
       "270        0    germanrlopez   \n",
       "271        1         StSenka   \n",
       "272        1      TruthPharm   \n",
       "273        1    MorganGodvin   \n",
       "274        0    SirajAHashmi   \n",
       "\n",
       "                                        mentioned_urls  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                     http://drugs.com  \n",
       "3                                                  NaN  \n",
       "4    https://twitter.com/halsey/status/123326171381...  \n",
       "..                                                 ...  \n",
       "270                                                NaN  \n",
       "271                                                NaN  \n",
       "272  https://twitter.com/TruthPharm/status/12355996...  \n",
       "273  https://twitter.com/MorganGodvin/status/123561...  \n",
       "274                                                NaN  \n",
       "\n",
       "[227972 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>username</th>\n      <th>text</th>\n      <th>date</th>\n      <th>geo</th>\n      <th>hashtags</th>\n      <th>tweet_id</th>\n      <th>mentions</th>\n      <th>permalink</th>\n      <th>replies</th>\n      <th>retweets</th>\n      <th>replies_to</th>\n      <th>mentioned_urls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>big wet smooshy kisses to you!!! i love u dear!!</td>\n      <td>2020-02-28 19:46:02+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478483939971072</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334784...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>amarcadia</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>list of meds and check it for drug interaction...</td>\n      <td>2020-02-28 19:45:26+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478333511479299</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334783...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1baldchick</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>gab has loads of side effects. i always try to...</td>\n      <td>2020-02-28 19:44:18+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478047212408834</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334780...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PajamaDaze</td>\n      <td>http://drugs.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>happy friday to u good sir!! and to your littl...</td>\n      <td>2020-02-28 19:42:32+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233477603866050560</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334776...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ChronicPainDad</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>this song says it all!</td>\n      <td>2020-02-28 08:34:04+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233309379497283584</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12333093...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>halsey</td>\n      <td>https://twitter.com/halsey/status/123326171381...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>total bullshit narrative.</td>\n      <td>2020-03-07 03:27:53+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1236131428892651520</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12361314...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>germanrlopez</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>doctors are not always right either and many w...</td>\n      <td>2020-03-05 23:36:05+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235710707993374721</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12357107...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>StSenka</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>yes x1000</td>\n      <td>2020-03-05 18:57:13+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640525740552192</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12356405...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>TruthPharm</td>\n      <td>https://twitter.com/TruthPharm/status/12355996...</td>\n    </tr>\n    <tr>\n      <th>273</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>very important thread. already hearing the anx...</td>\n      <td>2020-03-05 18:55:54+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640197926264832</td>\n      <td>@MorganGodvin</td>\n      <td>https://twitter.com/_tomdalton/status/12356401...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>MorganGodvin</td>\n      <td>https://twitter.com/MorganGodvin/status/123561...</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>is she trying to say he’s dumb or belittle him?</td>\n      <td>2020-03-02 01:30:36+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1234289973467254785</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12342899...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SirajAHashmi</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>227972 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fix text\n",
    "# for user in tqdm(list(before.keys())):\n",
    "#     before[user][\"text\"] = np.vectorize(decodeAndClean)(before[user][\"text\"])\n",
    "\n",
    "# for user in tqdm(list(after.keys())):\n",
    "#     after[user][\"text\"] = np.vectorize(decodeAndClean)(after[user][\"text\"])\n",
    "\n",
    "df[\"text\"] = np.vectorize(decodeAndClean)(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk style text cleaning\n",
    "df[\"text_nlp\"] = np.vectorize(preprocess)(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vader sentiment\n",
    "# for user in tqdm(list(before.keys())):\n",
    "#     before[user][\"sent\"] = np.vectorize(getSentiment)(before[user][\"text\"])\n",
    "\n",
    "# for user in tqdm(list(after.keys())):\n",
    "#     after[user][\"sent\"] = np.vectorize(getSentiment)(after[user][\"text\"])\n",
    "\n",
    "df[\"sent\"] = np.vectorize(getSentiment)(df[\"text_nlp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply emolex\n",
    "# for user in tqdm(list(before.keys())):\n",
    "#     before[user][[\"nrc_fear\", \n",
    "#         \"nrc_anger\", \n",
    "#         \"nrc_anticipation\", \n",
    "#         \"nrc_trust\", \n",
    "#         \"nrc_surprise\", \n",
    "#         \"nrc_positive\", \n",
    "#         \"nrc_negative\", \n",
    "#         \"nrc_sadness\", \n",
    "#         \"nrc_disgust\", \n",
    "#         \"nrc_joy\"]] = list(map(pandasMultiCategorySentiment, before[user][\"text\"])) #np.vectorize(pandasMultiCategorySentiment)(before[user][\"text\"])\n",
    "\n",
    "# for user in tqdm(list(after.keys())):\n",
    "#     after[user][[\"nrc_fear\", \n",
    "#         \"nrc_anger\", \n",
    "#         \"nrc_anticipation\", \n",
    "#         \"nrc_trust\", \n",
    "#         \"nrc_surprise\", \n",
    "#         \"nrc_positive\", \n",
    "#         \"nrc_negative\", \n",
    "#         \"nrc_sadness\", \n",
    "#         \"nrc_disgust\", \n",
    "#         \"nrc_joy\"]] = list(map(pandasMultiCategorySentiment, after[user][\"text\"])) #np.vectorize(pandasMultiCategorySentiment)(after[user][\"text\"])\n",
    "\n",
    "df[[\"nrc_fear\", \n",
    "    \"nrc_anger\", \n",
    "    \"nrc_anticipation\", \n",
    "    \"nrc_trust\", \n",
    "    \"nrc_surprise\", \n",
    "    \"nrc_positive\", \n",
    "    \"nrc_negative\", \n",
    "    \"nrc_sadness\", \n",
    "    \"nrc_disgust\", \n",
    "    \"nrc_joy\"]] = list(map(pandasMultiCategorySentiment, df[\"text_nlp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getSentiment(before[\"1baldchick\"][\"text\"][0])\n",
    "# list(map(getSentiment, before[\"1baldchick\"][\"text\"]))\n",
    "# before[\"1baldchick\"].drop(before[\"1baldchick\"][pd.isnull(before[\"1baldchick\"][\"text\"])].index)\n",
    "# before[file.stem]before[\"1baldchick\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 cat    username  \\\n",
       "0    before_pandemic  1baldchick   \n",
       "1    before_pandemic  1baldchick   \n",
       "2    before_pandemic  1baldchick   \n",
       "3    before_pandemic  1baldchick   \n",
       "4    before_pandemic  1baldchick   \n",
       "..               ...         ...   \n",
       "270   since_pandemic  _tomdalton   \n",
       "271   since_pandemic  _tomdalton   \n",
       "272   since_pandemic  _tomdalton   \n",
       "273   since_pandemic  _tomdalton   \n",
       "274   since_pandemic  _tomdalton   \n",
       "\n",
       "                                                  text  \\\n",
       "0     big wet smooshy kisses to you!!! i love u dear!!   \n",
       "1    list of meds and check it for drug interaction...   \n",
       "2    gab has loads of side effects. i always try to...   \n",
       "3    happy friday to u good sir!! and to your littl...   \n",
       "4                               this song says it all!   \n",
       "..                                                 ...   \n",
       "270                          total bullshit narrative.   \n",
       "271  doctors are not always right either and many w...   \n",
       "272                                          yes x1000   \n",
       "273  very important thread. already hearing the anx...   \n",
       "274     is she trying to say hes dumb or belittle him?   \n",
       "\n",
       "                          date  geo hashtags             tweet_id  \\\n",
       "0    2020-02-28 19:46:02+00:00  NaN      NaN  1233478483939971072   \n",
       "1    2020-02-28 19:45:26+00:00  NaN      NaN  1233478333511479299   \n",
       "2    2020-02-28 19:44:18+00:00  NaN      NaN  1233478047212408834   \n",
       "3    2020-02-28 19:42:32+00:00  NaN      NaN  1233477603866050560   \n",
       "4    2020-02-28 08:34:04+00:00  NaN      NaN  1233309379497283584   \n",
       "..                         ...  ...      ...                  ...   \n",
       "270  2020-03-07 03:27:53+00:00  NaN      NaN  1236131428892651520   \n",
       "271  2020-03-05 23:36:05+00:00  NaN      NaN  1235710707993374721   \n",
       "272  2020-03-05 18:57:13+00:00  NaN      NaN  1235640525740552192   \n",
       "273  2020-03-05 18:55:54+00:00  NaN      NaN  1235640197926264832   \n",
       "274  2020-03-02 01:30:36+00:00  NaN      NaN  1234289973467254785   \n",
       "\n",
       "          mentions                                          permalink replies  \\\n",
       "0              NaN  https://twitter.com/1baldchick/status/12334784...       0   \n",
       "1              NaN  https://twitter.com/1baldchick/status/12334783...       1   \n",
       "2              NaN  https://twitter.com/1baldchick/status/12334780...       1   \n",
       "3              NaN  https://twitter.com/1baldchick/status/12334776...       0   \n",
       "4              NaN  https://twitter.com/1baldchick/status/12333093...       0   \n",
       "..             ...                                                ...     ...   \n",
       "270            NaN  https://twitter.com/_tomdalton/status/12361314...       0   \n",
       "271            NaN  https://twitter.com/_tomdalton/status/12357107...       0   \n",
       "272            NaN  https://twitter.com/_tomdalton/status/12356405...       0   \n",
       "273  @MorganGodvin  https://twitter.com/_tomdalton/status/12356401...       1   \n",
       "274            NaN  https://twitter.com/_tomdalton/status/12342899...       0   \n",
       "\n",
       "     ...  nrc_fear nrc_anger nrc_anticipation nrc_trust  nrc_surprise  \\\n",
       "0    ...  0.096774  0.096774         0.096774  0.129032      0.096774   \n",
       "1    ...  0.102941  0.117647         0.132353  0.088235      0.088235   \n",
       "2    ...  0.100000  0.100000         0.106250  0.093750      0.100000   \n",
       "3    ...  0.108108  0.108108         0.108108  0.108108      0.081081   \n",
       "4    ...  0.125000  0.125000         0.125000  0.125000      0.000000   \n",
       "..   ...       ...       ...              ...       ...           ...   \n",
       "270  ...  0.100000  0.100000         0.100000  0.100000      0.100000   \n",
       "271  ...  0.098592  0.084507         0.098592  0.098592      0.112676   \n",
       "272  ...  0.000000  0.000000         0.000000  0.000000      0.000000   \n",
       "273  ...  0.116279  0.116279         0.116279  0.046512      0.116279   \n",
       "274  ...  0.111111  0.148148         0.148148  0.111111      0.074074   \n",
       "\n",
       "     nrc_positive  nrc_negative  nrc_sadness  nrc_disgust   nrc_joy  \n",
       "0        0.096774      0.096774     0.096774     0.096774  0.096774  \n",
       "1        0.088235      0.088235     0.102941     0.088235  0.102941  \n",
       "2        0.100000      0.100000     0.106250     0.100000  0.093750  \n",
       "3        0.081081      0.108108     0.108108     0.081081  0.108108  \n",
       "4        0.125000      0.125000     0.125000     0.000000  0.125000  \n",
       "..            ...           ...          ...          ...       ...  \n",
       "270      0.100000      0.100000     0.100000     0.100000  0.100000  \n",
       "271      0.084507      0.112676     0.126761     0.084507  0.098592  \n",
       "272      0.000000      0.000000     0.000000     0.000000  0.000000  \n",
       "273      0.093023      0.116279     0.093023     0.069767  0.116279  \n",
       "274      0.074074      0.074074     0.074074     0.074074  0.111111  \n",
       "\n",
       "[227972 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>username</th>\n      <th>text</th>\n      <th>date</th>\n      <th>geo</th>\n      <th>hashtags</th>\n      <th>tweet_id</th>\n      <th>mentions</th>\n      <th>permalink</th>\n      <th>replies</th>\n      <th>...</th>\n      <th>nrc_fear</th>\n      <th>nrc_anger</th>\n      <th>nrc_anticipation</th>\n      <th>nrc_trust</th>\n      <th>nrc_surprise</th>\n      <th>nrc_positive</th>\n      <th>nrc_negative</th>\n      <th>nrc_sadness</th>\n      <th>nrc_disgust</th>\n      <th>nrc_joy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>big wet smooshy kisses to you!!! i love u dear!!</td>\n      <td>2020-02-28 19:46:02+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478483939971072</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334784...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.129032</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n      <td>0.096774</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>list of meds and check it for drug interaction...</td>\n      <td>2020-02-28 19:45:26+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478333511479299</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334783...</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.102941</td>\n      <td>0.117647</td>\n      <td>0.132353</td>\n      <td>0.088235</td>\n      <td>0.088235</td>\n      <td>0.088235</td>\n      <td>0.088235</td>\n      <td>0.102941</td>\n      <td>0.088235</td>\n      <td>0.102941</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>gab has loads of side effects. i always try to...</td>\n      <td>2020-02-28 19:44:18+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478047212408834</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334780...</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.106250</td>\n      <td>0.093750</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.106250</td>\n      <td>0.100000</td>\n      <td>0.093750</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>happy friday to u good sir!! and to your littl...</td>\n      <td>2020-02-28 19:42:32+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233477603866050560</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334776...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.108108</td>\n      <td>0.108108</td>\n      <td>0.108108</td>\n      <td>0.108108</td>\n      <td>0.081081</td>\n      <td>0.081081</td>\n      <td>0.108108</td>\n      <td>0.108108</td>\n      <td>0.081081</td>\n      <td>0.108108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>this song says it all!</td>\n      <td>2020-02-28 08:34:04+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233309379497283584</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12333093...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>total bullshit narrative.</td>\n      <td>2020-03-07 03:27:53+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1236131428892651520</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12361314...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>doctors are not always right either and many w...</td>\n      <td>2020-03-05 23:36:05+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235710707993374721</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12357107...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.098592</td>\n      <td>0.084507</td>\n      <td>0.098592</td>\n      <td>0.098592</td>\n      <td>0.112676</td>\n      <td>0.084507</td>\n      <td>0.112676</td>\n      <td>0.126761</td>\n      <td>0.084507</td>\n      <td>0.098592</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>yes x1000</td>\n      <td>2020-03-05 18:57:13+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640525740552192</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12356405...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>273</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>very important thread. already hearing the anx...</td>\n      <td>2020-03-05 18:55:54+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640197926264832</td>\n      <td>@MorganGodvin</td>\n      <td>https://twitter.com/_tomdalton/status/12356401...</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.116279</td>\n      <td>0.116279</td>\n      <td>0.116279</td>\n      <td>0.046512</td>\n      <td>0.116279</td>\n      <td>0.093023</td>\n      <td>0.116279</td>\n      <td>0.093023</td>\n      <td>0.069767</td>\n      <td>0.116279</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>is she trying to say hes dumb or belittle him?</td>\n      <td>2020-03-02 01:30:36+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1234289973467254785</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12342899...</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.111111</td>\n      <td>0.148148</td>\n      <td>0.148148</td>\n      <td>0.111111</td>\n      <td>0.074074</td>\n      <td>0.074074</td>\n      <td>0.074074</td>\n      <td>0.074074</td>\n      <td>0.074074</td>\n      <td>0.111111</td>\n    </tr>\n  </tbody>\n</table>\n<p>227972 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"proc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}