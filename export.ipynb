{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd0c4a6cc1c2df5ddd62d6925b2a7bdee9abacf912eab37272999970e810b9642fd",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import html\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from emolexHelper import pandasMultiCategorySentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_DIR = \"before_pandemic\"\n",
    "AFTER_DIR = \"since_pandemic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the csv files from\n",
    "#before = dict()\n",
    "df = pd.DataFrame(columns=[\"cat\", \"username\", \"text\", \"date\", \"geo\", \"hashtags\", \"tweet_id\", \"mentions\", \"permalink\", \"replies\", \"retweets\", \"replies_to\", \"mentioned_urls\"])\n",
    "\n",
    "for file in list((Path(\".\") / BEFORE_DIR).glob('**/*.csv')):\n",
    "    #before[file.stem] = pd.read_csv(file)\n",
    "    user_df = pd.read_csv(file)\n",
    "    # BUG? why are there nan text rows?\n",
    "    # dropping them!\n",
    "    user_df = user_df.drop(user_df[pd.isnull(user_df[\"text\"])].index)\n",
    "    user_df[\"cat\"] = BEFORE_DIR\n",
    "    user_df[\"username\"] = file.stem\n",
    "    df = df.append(user_df)\n",
    "    #before[file.stem] = before[file.stem].drop(before[file.stem][pd.isnull(before[file.stem][\"text\"])].index)\n",
    "\n",
    "#after = dict()\n",
    "for file in list((Path(\".\") / AFTER_DIR).glob('**/*.csv')):\n",
    "    #after[file.stem] = pd.read_csv(file)\n",
    "    user_df = pd.read_csv(file)\n",
    "    # BUG? why are there nan text rows?\n",
    "    # dropping them!\n",
    "    user_df = user_df.drop(user_df[pd.isnull(user_df[\"text\"])].index)\n",
    "    user_df[\"cat\"] = AFTER_DIR\n",
    "    user_df[\"username\"] = file.stem\n",
    "    df = df.append(user_df)\n",
    "    #after[file.stem] = after[file.stem].drop(after[file.stem][pd.isnull(after[file.stem][\"text\"])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeAndClean(s:str):\n",
    "    out = html.unescape(s)\n",
    "    out = p.clean(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in filtered_words]\n",
    "    \n",
    "    return \" \".join(filtered_words) #convert stemmed words to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 cat    username  \\\n",
       "0    before_pandemic  1baldchick   \n",
       "1    before_pandemic  1baldchick   \n",
       "2    before_pandemic  1baldchick   \n",
       "3    before_pandemic  1baldchick   \n",
       "4    before_pandemic  1baldchick   \n",
       "..               ...         ...   \n",
       "270   since_pandemic  _tomdalton   \n",
       "271   since_pandemic  _tomdalton   \n",
       "272   since_pandemic  _tomdalton   \n",
       "273   since_pandemic  _tomdalton   \n",
       "274   since_pandemic  _tomdalton   \n",
       "\n",
       "                                                  text  \\\n",
       "0     big wet smooshy kisses to you!!! i love u dear!!   \n",
       "1    list of meds and check it for drug interaction...   \n",
       "2    gab has loads of side effects. i always try to...   \n",
       "3    happy friday to u good sir!! and to your littl...   \n",
       "4                               this song says it all!   \n",
       "..                                                 ...   \n",
       "270                          total bullshit narrative.   \n",
       "271  doctors are not always right either and many w...   \n",
       "272                                         yes x1000    \n",
       "273  very important thread. already hearing the anx...   \n",
       "274    is she trying to say he’s dumb or belittle him?   \n",
       "\n",
       "                          date  geo hashtags             tweet_id  \\\n",
       "0    2020-02-28 19:46:02+00:00  NaN      NaN  1233478483939971072   \n",
       "1    2020-02-28 19:45:26+00:00  NaN      NaN  1233478333511479299   \n",
       "2    2020-02-28 19:44:18+00:00  NaN      NaN  1233478047212408834   \n",
       "3    2020-02-28 19:42:32+00:00  NaN      NaN  1233477603866050560   \n",
       "4    2020-02-28 08:34:04+00:00  NaN      NaN  1233309379497283584   \n",
       "..                         ...  ...      ...                  ...   \n",
       "270  2020-03-07 03:27:53+00:00  NaN      NaN  1236131428892651520   \n",
       "271  2020-03-05 23:36:05+00:00  NaN      NaN  1235710707993374721   \n",
       "272  2020-03-05 18:57:13+00:00  NaN      NaN  1235640525740552192   \n",
       "273  2020-03-05 18:55:54+00:00  NaN      NaN  1235640197926264832   \n",
       "274  2020-03-02 01:30:36+00:00  NaN      NaN  1234289973467254785   \n",
       "\n",
       "          mentions                                          permalink replies  \\\n",
       "0              NaN  https://twitter.com/1baldchick/status/12334784...       0   \n",
       "1              NaN  https://twitter.com/1baldchick/status/12334783...       1   \n",
       "2              NaN  https://twitter.com/1baldchick/status/12334780...       1   \n",
       "3              NaN  https://twitter.com/1baldchick/status/12334776...       0   \n",
       "4              NaN  https://twitter.com/1baldchick/status/12333093...       0   \n",
       "..             ...                                                ...     ...   \n",
       "270            NaN  https://twitter.com/_tomdalton/status/12361314...       0   \n",
       "271            NaN  https://twitter.com/_tomdalton/status/12357107...       0   \n",
       "272            NaN  https://twitter.com/_tomdalton/status/12356405...       0   \n",
       "273  @MorganGodvin  https://twitter.com/_tomdalton/status/12356401...       1   \n",
       "274            NaN  https://twitter.com/_tomdalton/status/12342899...       0   \n",
       "\n",
       "    retweets      replies_to  \\\n",
       "0          0       amarcadia   \n",
       "1          0      1baldchick   \n",
       "2          0      PajamaDaze   \n",
       "3          0  ChronicPainDad   \n",
       "4          0          halsey   \n",
       "..       ...             ...   \n",
       "270        0    germanrlopez   \n",
       "271        1         StSenka   \n",
       "272        1      TruthPharm   \n",
       "273        1    MorganGodvin   \n",
       "274        0    SirajAHashmi   \n",
       "\n",
       "                                        mentioned_urls  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                     http://drugs.com  \n",
       "3                                                  NaN  \n",
       "4    https://twitter.com/halsey/status/123326171381...  \n",
       "..                                                 ...  \n",
       "270                                                NaN  \n",
       "271                                                NaN  \n",
       "272  https://twitter.com/TruthPharm/status/12355996...  \n",
       "273  https://twitter.com/MorganGodvin/status/123561...  \n",
       "274                                                NaN  \n",
       "\n",
       "[227972 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>username</th>\n      <th>text</th>\n      <th>date</th>\n      <th>geo</th>\n      <th>hashtags</th>\n      <th>tweet_id</th>\n      <th>mentions</th>\n      <th>permalink</th>\n      <th>replies</th>\n      <th>retweets</th>\n      <th>replies_to</th>\n      <th>mentioned_urls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>big wet smooshy kisses to you!!! i love u dear!!</td>\n      <td>2020-02-28 19:46:02+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478483939971072</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334784...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>amarcadia</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>list of meds and check it for drug interaction...</td>\n      <td>2020-02-28 19:45:26+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478333511479299</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334783...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1baldchick</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>gab has loads of side effects. i always try to...</td>\n      <td>2020-02-28 19:44:18+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233478047212408834</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334780...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PajamaDaze</td>\n      <td>http://drugs.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>happy friday to u good sir!! and to your littl...</td>\n      <td>2020-02-28 19:42:32+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233477603866050560</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12334776...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ChronicPainDad</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>before_pandemic</td>\n      <td>1baldchick</td>\n      <td>this song says it all!</td>\n      <td>2020-02-28 08:34:04+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1233309379497283584</td>\n      <td>NaN</td>\n      <td>https://twitter.com/1baldchick/status/12333093...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>halsey</td>\n      <td>https://twitter.com/halsey/status/123326171381...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>total bullshit narrative.</td>\n      <td>2020-03-07 03:27:53+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1236131428892651520</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12361314...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>germanrlopez</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>doctors are not always right either and many w...</td>\n      <td>2020-03-05 23:36:05+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235710707993374721</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12357107...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>StSenka</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>yes x1000</td>\n      <td>2020-03-05 18:57:13+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640525740552192</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12356405...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>TruthPharm</td>\n      <td>https://twitter.com/TruthPharm/status/12355996...</td>\n    </tr>\n    <tr>\n      <th>273</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>very important thread. already hearing the anx...</td>\n      <td>2020-03-05 18:55:54+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1235640197926264832</td>\n      <td>@MorganGodvin</td>\n      <td>https://twitter.com/_tomdalton/status/12356401...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>MorganGodvin</td>\n      <td>https://twitter.com/MorganGodvin/status/123561...</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>since_pandemic</td>\n      <td>_tomdalton</td>\n      <td>is she trying to say he’s dumb or belittle him?</td>\n      <td>2020-03-02 01:30:36+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1234289973467254785</td>\n      <td>NaN</td>\n      <td>https://twitter.com/_tomdalton/status/12342899...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SirajAHashmi</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>227972 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = np.vectorize(decodeAndClean)(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_nlp\"] = np.vectorize(preprocess)(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-9-85ea215eeb4a>:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"date\"] = pd.to_datetime(df[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "df[\"date\"] = list(map(lambda s: s[:-3] + s[-2:], df[\"date\"]))\n",
    "df = df.drop(df[df[\"date\"]==\"dte\"].index) # BUG? For some reason some date colums contain the value \"dte\"\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(pd.Grouper(key=\"date\", freq='5d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing 2019_10_01 (2714 lines)\n",
      "Processing 2019_10_06 (2947 lines)\n",
      "Processing 2019_10_11 (2926 lines)\n",
      "Processing 2019_10_16 (2652 lines)\n",
      "Processing 2019_10_21 (2580 lines)\n",
      "Processing 2019_10_26 (2608 lines)\n",
      "Processing 2019_10_31 (2810 lines)\n",
      "Processing 2019_11_05 (2768 lines)\n",
      "Processing 2019_11_10 (2762 lines)\n",
      "Processing 2019_11_15 (2810 lines)\n",
      "Processing 2019_11_20 (3267 lines)\n",
      "Processing 2019_11_25 (3099 lines)\n",
      "Processing 2019_11_30 (3202 lines)\n",
      "Processing 2019_12_05 (3398 lines)\n",
      "Processing 2019_12_10 (3231 lines)\n",
      "Processing 2019_12_15 (3090 lines)\n",
      "Processing 2019_12_20 (2964 lines)\n",
      "Processing 2019_12_25 (2994 lines)\n",
      "Processing 2019_12_30 (2594 lines)\n",
      "Processing 2020_01_04 (2771 lines)\n",
      "Processing 2020_01_09 (3709 lines)\n",
      "Processing 2020_01_14 (3453 lines)\n",
      "Processing 2020_01_19 (3285 lines)\n",
      "Processing 2020_01_24 (3899 lines)\n",
      "Processing 2020_01_29 (4122 lines)\n",
      "Processing 2020_02_03 (3645 lines)\n",
      "Processing 2020_02_08 (4170 lines)\n",
      "Processing 2020_02_13 (3790 lines)\n",
      "Processing 2020_02_18 (3547 lines)\n",
      "Processing 2020_02_23 (3806 lines)\n",
      "Processing 2020_02_28 (3287 lines)\n",
      "Processing 2020_03_04 (4481 lines)\n",
      "Processing 2020_03_09 (5805 lines)\n",
      "Processing 2020_03_14 (6000 lines)\n",
      "Processing 2020_03_19 (4503 lines)\n",
      "Processing 2020_03_24 (4928 lines)\n",
      "Processing 2020_03_29 (4520 lines)\n",
      "Processing 2020_04_03 (4749 lines)\n",
      "Processing 2020_04_08 (4536 lines)\n",
      "Processing 2020_04_13 (4231 lines)\n",
      "Processing 2020_04_18 (4176 lines)\n",
      "Processing 2020_04_23 (3538 lines)\n",
      "Processing 2020_04_28 (3756 lines)\n",
      "Processing 2020_05_03 (3808 lines)\n",
      "Processing 2020_05_08 (4093 lines)\n",
      "Processing 2020_05_13 (4201 lines)\n",
      "Processing 2020_05_18 (3312 lines)\n",
      "Processing 2020_05_23 (3144 lines)\n",
      "Processing 2020_05_28 (3378 lines)\n",
      "Processing 2020_06_02 (4567 lines)\n",
      "Processing 2020_06_07 (4275 lines)\n",
      "Processing 2020_06_12 (3898 lines)\n",
      "Processing 2020_06_17 (4304 lines)\n",
      "Processing 2020_06_22 (4294 lines)\n",
      "Processing 2020_06_27 (4505 lines)\n",
      "Processing 2020_07_02 (4037 lines)\n",
      "Processing 2020_07_07 (4418 lines)\n",
      "Processing 2020_07_12 (4036 lines)\n",
      "Processing 2020_07_17 (5082 lines)\n",
      "Processing 2020_07_22 (4761 lines)\n",
      "Processing 2020_07_27 (3286 lines)\n"
     ]
    }
   ],
   "source": [
    "path = (Path(\".\") / \"export\")\n",
    "for name, group in grouped: #tqdm(list(grouped)):\n",
    "    fname = str(name).split()[0].replace(\"-\", \"_\")\n",
    "    text = list(group[\"text\"].values)\n",
    "    fileHandle = open(str((path / fname).absolute()) + \".txt\", \"w\")\n",
    "    fileHandle.write(\"\\n\".join(text))\n",
    "    # fileHandle.writelines(text)\n",
    "    fileHandle.close()\n",
    "    print(f\"Processing {fname} ({len(text)} lines)\")\n",
    "    # print(fname)\n",
    "    # print(text)\n",
    "    # print(len(text))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}